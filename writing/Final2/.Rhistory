dt <- dt[freq>1]
bigrams <- rbindlist(list(bigrams, dtfinal))
bigrams <- rbindlist(list(bigrams, dt))
unigrams <- rbindlist(list(unigrams, dt))
unigrams <- data.table(ngram=character(), freq=numeric())
unigrams <- rbindlist(list(unigrams, dt))
dim(unigrams[freq>2])
t1 <- proc.time()
t2 <- proc.time()
loginfo('Created list of %d ngrams in %.2f seconds',
length(n1), dim(unigrams)[1], t2[3]-t1[3])
library(logging)
basicConfig()
loginfo('Created list of %d ngrams in %.2f seconds',
length(n1), dim(unigrams)[1], t2[3]-t1[3])
t1 <- proc.time()
t2 <- proc.time()
t2[3]-t1[3]
rm(list=ls())
debugSource("train2.R")
train()
debugSource("train2.R")
train()
debugSource("train2.R")
train()
debugSource("train2.R")
train()
debugSource("train2.R")
train()
train()
train()
train()
x
class(x)
debugSource("train2.R")
train()
debugSource("train2.R")
train()
debugSource("train2.R")
train()
print(n)
print(ngramSingle)
print(concatenator)
print(include.all)
print(FUN)
train()
all_ngrams
train()
print(width)
length(text)
text
start
end
length(tokens)
print(n)
length(tokens)
print(width)
length(tokens) - width + 1
width
new_ngrams
i
tokens[(i + 1):(length(tokens) - width + 1 +
i)]
new_ngrams
debugSource("train2.R")
train()
debugSource("train2.R")
train()
ngrams("The quick brown fox jumped over the lazy dog.", n=2)
ngrams("The quick brown fox jumped over the lazy dog.", n=2, concatenator=" ")
debugSource("train2.R")
train()
print(tokens)
print(simplify)
print(text)
train()
print(files)
debugSource("train2.R")
train()
?bigrams
debugSource("train2.R")
train()
debugSource("train2.R")
train()
txt = readLines('../data/sample/en_US.twitter.sample3.txt', n=20,skipNul = TRUE, encoding="UTF-8")
txt
ngrams(txt)
ngrams(txt, concatenator=" ")
identical(ngrams(txt, concatenator=" "), tokenize(txt, concatenator=" "))
tokenize(txt, concatenator=" ")
tokenize(txt, concatenator=" ", n=2)
identical(ngrams(txt, concatenator=" "), tokenize(txt, concatenator=" ", n=2))
identical(unlist(ngrams(txt, concatenator=" ")), unlist(tokenize(txt, concatenator=" ", n=2))
)
length(unlist(ngrams(txt, concatenator=" ")))
length(tokenize(txt, concatenator=" ", n=2))
length(unlist(tokenize(txt, concatenator=" ", n=2))
)
tmp1=t(unlist(ngrams(txt, concatenator=" ")))
tmp2=t(unlist(tokenize(txt, concatenator=" ", n=2)))
tmp1
tmp2
txt
rm(list=ls())
debugSource("train2.R")
train()
debugSource("train2.R")
train()
debugSource("train2.R")
train()
debugSource("train2.R")
train()
head(unigrams)
head(bigrams)
?readLines
head(bigrams, n=50)
head(unigrams, n=50)
names(unigrams)
names(bigrams)
debugSource("train2.R")
train()
debugSource("train2.R")
train()
head(unigrams, n=50)
head(bigrams, n=50)
system(paste("if [ -e unigrams.rds ] ; then mv -f unigrams.rds unigrams.rds.old\n",
"mv -f bigrams.rds bigrams.rds.old\n",
"mv -f trigrams.rds trigrams.rds.old; fi\n", sep=""))
system(paste("bash -c 'if [ -e unigrams.rds ] ; then mv -f unigrams.rds unigrams.rds.old;",
"mv -f bigrams.rds bigrams.rds.old;",
"mv -f trigrams.rds trigrams.rds.old; fi'\n", sep=" "))
debugSource("train2.R")
train()
head(unigrams, n=50)
head(bigrams, n=50)
count1 <- unigrams[ bigrams, freq ]
debugSource("train2.R")
train()
debugSource("train2.R")
train()
test <- = unigrams[,freq:=sum(freq), by=ngram]
test = unigrams[,freq:=sum(freq), by=ngram]
dim(test)
dim(unigrams)
test = unigrams[,freq=sum(freq), by=ngram]
test = unigrams[,list(freq=sum(freq)), by=ngram]
dim(test)
test = unigrams[,.(freq=sum(freq)), by=ngram]
dim(test)
length(unique(unigrams[,freq]))
length(unique(unigrams[,ngram]))
unigrams[,ngram="the"]
unigrams[ngram=="the"]
test[ngram=="the"]
2948591*3
unigrams[ngram=="an"]
debugSource("train2.R")
train()
files[i]
unigrams[ngram=="the"]
dt[ngram=="the"]
unigrams[ngram=="the"]
dt[ngram=="the"]
1859353/2
unigrams[ngram=="the"]
unigrams[ngram=="the"]
dt[ngram=="the"]
unigrams[ngram=="the"]
unigrams[ngram=="the"]
unigrams[ngram=="the"]
937489+1859353+151749
length(unique(unigrams[,ngram]))
unigrams[ngram=="an"]
dim(unigrams)
count1 <- unigrams[ bigrams, freq ]
debugSource("test.R")
test()
debugSource("test.R")
test()
rm(list=ls())
debugSource("test.R")
test()
matches
debugSource("train.R")
train()
debugSource("train2.R")
train()
rm(list=ls())
debugSource("test.R")
test()
matches
head(threegrams)
threegrams[sort(-prob)]
head(threegrams)
max(threegrams[,prob])
threegrams = threegrams[sort(-prob)]
head(threegrams)
threegrams[order(-prob)]
head(threegrams)
rm(list=ls())
debugSource("test.R")
debugSource("train2.R")
train()
rm(list=ls())
debugSource("test.R")
test()
matches
matches
matches
matches[pword=="happiest"]
matches
matches[pword=="defense"]
rm(list=ls())
t1 <- proc.time(); txt = readLines('../data/sample/en_US.twitter.sample5.txt',skipNul = TRUE, encoding="UTF-8"); n1=unlist(tokenize(txt, n=1)); t2 <- proc.time()
rm(list=ls())
debugSource("train.R")
train()
?tbl
?saveSQLite
library(RSQLite)
library(RSQLite.extfuns)
devtools::install_github("RcppCore/Rcpp")
devtools::install_github("rstats-db/DBI")
devtools::install_github("rstats-db/RSQLite")
latest <- "http://www.sqlite.org/2015/sqlite-amalgamation-3080802.zip"
tmp <- tempfile()
download.file(latest, tmp)
unzip(tmp, exdir = "src/", junkpaths = TRUE)
unlink("src/shell.c")
?unzip
?tbl
grams <- c("unigrams"=1, "bigrams"=2, "trigrams"=3, "quadrigrams"=4,
"quintigrams"=5)
grams[1]
grams[1]$name
grams <- c("unigrams", "bigrams", "trigrams", "quadrigrams",
"quintigrams")
grams[1]
?set
?base::set
?saveRDS
?readRDS
?saveSQLite
library(RSQLite)
library(Rcpp)
library(DBI)
install.packages("RSQLite")
library(RSQLite)
debugSource("train.R")
install.packages("RSQLite.extfuns")
debugSource("train.R")
train()
debugSource("train.R")
train()
?db_location
?dplyr:::db_location
library(RSQLite)
?dplyr:::db_location
?src_sqlite
debugSource("train.R")
train()
saveRDS(unigrams, "unigrams.rds")
debugSource("train.R")
train()
dim(bigrams)
bigrams <- readRDS("bigrams1.rds")
dim(bigrams)
bigrams[, c("word1", "pword") := tstrsplit(ngram, " ", fixed=TRUE)]
bigrams <- bigrams[pword != word1]
dim(bigrams)
bigrams <- readRDS("bigrams1.rds")
bigrams[, c("word1", "pword") := tstrsplit(ngram, " ", fixed=TRUE)]
head(bigrams)
debugSource("train.R")
train()
head(dt)
head(dt, n=50)
j
head(tokenize(txt[1:10],n=2))
txt = readLines(files[i],skipNul = TRUE, encoding="UTF-8")
head(tokenize(txt[1:10],n=2))
head(tokenize(txt[1:10],n=2, concatenator = " "))
debugSource("train.R")
train()
path
src<- src_sqlite(path=paste0("./",name, ".sqlite"), create=TRUE)
debugSource("train.R")
train()
train()
debugSource("predict.R")
predict()
debugSource("test.R")
test()
head(onegrams)
phrase
uWords
uWords[uWords!=""]
uWords <- simplify2array(str_split(phrase, pattern="[ .,;:?!-]"))
debugSource("test.R")
test()
which(uWords=="")
uWords
uWords <- simplify2array(str_split(phrase, pattern="[.,;:?!-]"))
uWords
uWords[length(uWords)]
uWords <- simplify2array(str_split(uWords[length(uWords)], pattern="[ .,;:?!-]"))
uWords
?str_trim
debugSource("test.R")
test()
length(uuWords)
length(uWords)
uWords
tokens
tokens=tail(uWords, n=4)
tokens
debugSource("test.R")
test()
tokens
matches
nrow(matches)
debugSource("test.R")
test()
uWords
names(fivegrams)
paste(tokens[1:4], collapse=" ")
head(fivegrams)
headtail(fivegrams)
tail(fivegrams)
fivegrams=fivegrams[order(-prob)]
head(fivegrams)
head(fivegrams, n=50)
fivegrams=fivegrams[order(-prob, freq)]
head(fivegrams, n=50)
fivegrams=fivegrams[order(-prob, -freq)]
head(fivegrams, n=50)
debugSource("test.R")
test()
matches
matches
matches[order(-prob,-freq)]
debugSource("test.R")
test()
matches
matches
matches
matches
matches[pword=="defense"]
debugSource("train.R")
train()
debugSource("train.R")
train()
dt[ngram=="but the defense"]
head(dt)
j
dt=data.table(ngram=unlist(tokenize(txt, n=j, concatenator = " ")))
txt = readLines(files[i],skipNul = TRUE, encoding="UTF-8")
dt=data.table(ngram=unlist(tokenize(txt, n=j, concatenator = " ")))
head(dt)
dt[ngram=="but the defense"]
?grep
grep("but the defense", txt)
dim(txt)
txt = readLines(files[i],skipNul = TRUE, encoding="UTF-8")
dim(txt)
txt
class(txt)
length(txt)
grep("opera wine thoroughbred", txt)
grep("the history of", txt)
txt = readLines(files[i],skipNul = TRUE)
length(txt)
txt[length(txt)]
txt = readLines(files[i])
length(txt)
txt = readLines("../data/en_US/en_US.news.txt")
txt = readLines("../data/en_US/en_US.news.txt", skipNul=TRUE)
txt = readLines("../data/en_US/en_US.news.txt", skipNul=TRUE, encoding="UTF-16")
length(txt)
txt = readLines("../data/en_US/en_US.news.txt", skipNul=TRUE, encoding="ascii")
length(txt)
txt = readLines("../data/sample/test.txt", skipNul=TRUE, encoding="ascii")
length(txt)
txt = readLines("../data/sample/en_US.news.sample5.txt", skipNul=TRUE)
length(txt)
txt = readLines("../data/sample/en_US.blogs.sample5.txt", skipNul=TRUE)
length(txt)
txt = readLines("../data/sample/en_US.twitter.sample5.txt", skipNul=TRUE)
length(txt)
debugSource("train.R")
rm(list=ls())
debugSource("train.R")
train()
debugSource("train.R")
train()
debugSource("test.R")
test()
matches
matches
matches
matches
matches[pword=="defense"]
matches[,prob]/matches[,freq]
matches[,tfidf:=prob/freq]
matches=matches[order(-tfidf)]
matches
6.4E-5/2
6.4E-5/2.0
9.11E-3/281
matches[pword=="defense"]
matches[,tfidf:=(prob*100.)/freq]
.9/281
.91142/281
1./3.243489e-05
bigrams[word1=="but" & pword=="the"]
names(bigrams)
head(bigrams)
twograms[word1=="but" & pword=="the"]
setkey(onegrams, pword)
names(onegrams)
names(twograms)
onegrams <<- readRDS("unigrams.rds")
names(onegram)
names(onegrams)
unigrams <- readRDS("../data/unigrams1.rds")
names(unigrams)
debugSource("train.R")
debugSource("train.R")
train()
names(unigrams)
rm(list=ls())
unigrams <- readRDS("../data/unigrams1.rds")
unigrams[, prob := freq/sum(freq)]
setnames(DT,"ngram","pword")
unigrams <- unigrams[order(-prob)]
saveRDS(unigrams, file="unigrams.rds")
unigrams <- readRDS("../data/unigrams1.rds")
unigrams[, prob := freq/sum(freq)]
setnames(unigrams,"ngram","pword")
unigrams <- unigrams[order(-prob)]
saveRDS(unigrams, file="unigrams.rds")
debugSource("test.R")
test()
onegrams[pword=="defense"]
setkey(onegrams, pword)
setkey(trigrams, pword)
setkey(threegrams, pword)
setkey(matches, pword)
tmp =matches[onegrams]
tmp
tmp =onegrams[matches]
tmp
dim(matches)
head(matches[order(-prob,-freq)])
head(tmp[order(-prob,-freq)])
head(tmp[order(-i.prob,-i.freq)])
tmp[,tfidf:=i.prob/prob]
head(tmp[order(-tfidf)])
tmp[pword=="defense"]
matches
uWords
tokens = tail(uWords, n=2)
matches2 <- threegrams[word12==paste(tokens[1:2], collapse=" "),]
matches2
matches2=matches2[order(-prob,-freq)]
head(matches2)
matches2[pword=="grocery"]
matches2[pword=="mall"]
matches2[pword=="beach"]
matches
matches
matches
tokens = tail(uWords, n=2)
matches2 <- threegrams[word12==paste(tokens[1:2], collapse=" "),]
matches2=matches2[order(-prob,-freq)]
head(matches2)
matches2
matches
matches[pword=="bad"]
matches
tokens = tail(uWords, n=3)
matches <- fourgrams[word123==paste(tokens[1:3], collapse=" "),]
matches
matches=matches[order(-prob,-freq)]
matches
Q[6] = "Well I'm pretty sure my granny has some old bagpipes in her garage I'll dust them off and be on my"
shiny::runApp()
setwd("../writing")
setwd("./Final")
install.packages('knitr', dependencies = TRUE)
library(ggplot2)
install.packages(ggplot2)
install.packages("ggplot2")
date: 8/16/2015
date: August 16, 2015
First Slide
date: 8/162015
date: 8/16/2015
library(knitr)
- Bullet 2
- Bullet 3
Slide With Code
Capstone_Final_Slides
Capstone_Final_Slides
require(devtools)
install_github("slidify", "ramnathv")
install_github("ramnathv/slidifyLibraries")
library(slidify)
setwd("../Final2")
slidify("index.Rmd")
slidify("capstone_final_slides.Rmd")
devtools::install_github('muschellij2/slidify')
